# M3.2 Completed: Generation Worker

## Overview

Successfully implemented the Generation Worker using OpenAI's API with gpt-5-nano model for generating structured documentation. The worker uses OpenAI's structured output feature to ensure consistent, high-quality documentation generation.

## Implementation Summary

### Architecture

The generation worker is a Python-based FastAPI service that:
- Uses OpenAI's gpt-5-nano model with structured output
- Implements concurrent batch processing (default: 10 parallel requests)
- Includes retry logic with exponential backoff for rate limiting
- Tracks token usage and calculates costs accurately
- Emits structured events for progress tracking

### Key Components

#### 1. Token Counter (`app/token_counter.py`)
- Accurate token counting using tiktoken
- Cost calculation for input and output tokens
- Smart truncation to stay within token limits
- Support for message-based token counting

**Pricing:**
- Input: $0.15 per 1M tokens
- Output: $0.60 per 1M tokens

#### 2. Prompt Builder (`app/prompt_builder.py`)
- Constructs prompts from SymbolContext
- Includes language, signature, calls, called_by, cluster info
- Automatically truncates long signatures
- Limits relational data to prevent token overflow

**Prompt Template:**
```
You are documenting a {language} codebase.

Symbol: {name}
Kind: {kind}
File: {file_path}
Signature: {signature}
Calls: {calls}
Called by: {called_by}
Related to: {cluster_label}
Importance: {centrality} (0-1 scale)

Write 1-2 sentences describing what this symbol does.
Be concise and precise. Focus on purpose, not implementation.
```

#### 3. OpenAI Client (`app/openai_client.py`)
- Uses OpenAI's structured output API (beta.chat.completions.parse)
- Implements retry with exponential backoff (3 attempts)
- Handles rate limiting (429 errors)
- Tracks actual token usage from API response
- Returns structured DocumentationOutput

#### 4. Documentation Generator (`app/generator.py`)
- Concurrent batch processing with semaphore control
- Progress tracking with optional callbacks
- Partial failure handling (continues on errors)
- Collects warnings for failed symbols
- Returns comprehensive results with metrics

#### 5. FastAPI Application (`app/main.py`)
- `/health` endpoint for readiness checks
- `/generate` endpoint for batch documentation
- CORS middleware for cross-origin requests
- Lifecycle management for client initialization
- Event emission for progress tracking

#### 6. Event System (`app/events.py`)
- `generation.started.v1`: Job initiated
- `generation.symbol_documented.v1`: Individual symbol completed
- `generation.completed.v1`: Job finished with totals

### API Endpoints

#### GET /health

**Response:**
```json
{
  "status": "healthy",
  "model": "gpt-5-nano",
  "ready": true
}
```

#### POST /generate

**Request:**
```json
{
  "job_id": "job_123",
  "symbols": [
    {
      "symbol_id": "sym_1",
      "context": {
        "symbol_id": "sym_1",
        "name": "calculate_total",
        "kind": "function",
        "language": "python",
        "file_path": "src/utils.py",
        "signature": "def calculate_total(items: list[int]) -> int",
        "calls": ["sum", "len"],
        "called_by": ["main"],
        "imports": ["typing"],
        "related_symbols": ["validate_items"],
        "cluster_label": "math utilities",
        "centrality": 0.75
      }
    }
  ]
}
```

**Response:**
```json
{
  "documented_symbols": [
    {
      "symbol_id": "sym_1",
      "summary": "Calculates the total sum of integer items in a list.",
      "tokens_used": 87
    }
  ],
  "total_tokens": 87,
  "total_cost": 0.000013
}
```

## Project Structure

```
workers/generation/
├── app/
│   ├── __init__.py          # Package initialization
│   ├── config.py            # Configuration from environment
│   ├── schemas.py           # Pydantic models
│   ├── token_counter.py     # Token counting with tiktoken
│   ├── prompt_builder.py    # Prompt construction
│   ├── openai_client.py     # OpenAI API wrapper
│   ├── generator.py         # Batch processing
│   ├── events.py            # Event emission
│   └── main.py              # FastAPI application
├── tests/
│   ├── __init__.py
│   ├── test_token_counter.py
│   ├── test_prompt_builder.py
│   └── test_api.py
├── pyproject.toml           # Python dependencies
├── Dockerfile               # Container build
├── docker-compose.yml       # Local orchestration
├── setup.sh                 # Setup script
├── run.sh                   # Run script
├── .env.example             # Environment template
├── .gitignore               # Git ignore rules
├── README.md                # Full documentation
└── QUICKSTART.md            # Quick setup guide
```

## Dependencies

### Core
- `openai>=1.0.0` - OpenAI API client with structured output
- `tiktoken>=0.5.0` - Token counting
- `tenacity>=8.2.0` - Retry logic with exponential backoff
- `fastapi>=0.104.0` - Web framework
- `uvicorn[standard]>=0.24.0` - ASGI server
- `pydantic>=2.5.0` - Data validation
- `pydantic-settings>=2.0.0` - Settings management

### Development
- `pytest>=7.4.0` - Testing framework
- `pytest-asyncio>=0.21.0` - Async test support
- `pytest-mock>=3.12.0` - Mocking utilities
- `httpx>=0.25.0` - HTTP client for testing

## Testing

Comprehensive test suite covering:
- Token counting accuracy
- Cost calculation
- Prompt construction and truncation
- API endpoint contracts
- Error handling and retries

Run tests:
```bash
source .venv/bin/activate
pytest
pytest --cov=app --cov-report=html
```

## Configuration

Environment variables:
- `OPENAI_API_KEY` (required) - OpenAI API key
- `MODEL_NAME` - Model to use (default: gpt-5-nano)
- `MAX_CONCURRENT_REQUESTS` - Parallel requests (default: 10)
- `MAX_RETRIES` - Retry attempts (default: 3)
- `MAX_PROMPT_TOKENS` - Token limit per prompt (default: 2000)
- `INPUT_TOKEN_PRICE` - Input pricing (default: 0.15)
- `OUTPUT_TOKEN_PRICE` - Output pricing (default: 0.60)
- `PORT` - Server port (default: 8003)

## Features

### ✅ Structured Output
Uses OpenAI's structured output API to ensure consistent JSON responses with defined schema.

### ✅ Smart Truncation
Automatically truncates prompts exceeding 2000 tokens while preserving essential information.

### ✅ Concurrent Processing
Processes up to 10 symbols in parallel with semaphore-based concurrency control.

### ✅ Retry Logic
Exponential backoff retry for transient errors and rate limits.

### ✅ Cost Tracking
Accurate token counting and cost calculation per request and per batch.

### ✅ Progress Events
Emits structured events for real-time progress tracking.

### ✅ Partial Failures
Continues processing on individual failures, collecting warnings.

### ✅ Health Checks
Verifies API connectivity and readiness.

## Quick Start

1. **Setup:**
   ```bash
   cd workers/generation
   ./setup.sh
   ```

2. **Configure:**
   ```bash
   export OPENAI_API_KEY=your-key-here
   ```

3. **Run:**
   ```bash
   ./run.sh
   ```

4. **Test:**
   ```bash
   curl http://localhost:8003/health
   ```

## Docker Deployment

```bash
# Build
docker build -t doctown-generation-worker .

# Run
docker run -p 8003:8003 \
  -e OPENAI_API_KEY=your-key \
  doctown-generation-worker

# Or use docker-compose
export OPENAI_API_KEY=your-key
docker-compose up
```

## Integration with Pipeline

The generation worker integrates into the Doctown pipeline:

1. **Assembly Worker** creates symbol contexts
2. **Generation Worker** generates documentation from contexts
3. **Packer Worker** (M3.3) will package results into .docpack

The website can call the generation worker with symbol contexts from the assembly worker's output.

## TypeScript Integration

Added `GenerationClient` to `website/src/lib/api-client.ts`:

```typescript
const client = new GenerationClient('http://localhost:8003');
const health = await client.health();
const response = await client.generate({
  job_id: 'job_123',
  symbols: symbolInputs
});
```

## Next Steps

1. **M3.3: Packer Worker** - Package documented symbols into .docpack format
2. **M3.4: Builder Integration** - Integrate all workers into builder pipeline
3. **M3.5: RunPod Deployment** - Deploy to production
4. **Testing** - Integration tests with real OpenAI API (gated)

## Performance Characteristics

- **Latency**: ~1-3 seconds per symbol (depends on OpenAI API)
- **Throughput**: ~10 symbols per second (with max concurrency)
- **Cost**: ~$0.0001-0.0003 per symbol (varies with signature length)
- **Scalability**: Horizontally scalable (deploy multiple instances)

## Monitoring & Observability

Events provide observability:
- Track progress with `symbol_documented` events
- Monitor costs with token usage in events
- Detect failures with warnings in `completed` event
- Measure latency with duration_ms

## Notes

- Uses gpt-5-nano model (can be changed via MODEL_NAME env var)
- Structured output ensures consistent JSON responses
- Automatically handles rate limiting with exponential backoff
- Partial failures don't stop the batch
- Token counting uses tiktoken for accuracy
- Prompts include semantic context (cluster labels, centrality)

---

**Status:** ✅ Complete and ready for integration  
**Date:** 2025-11-23  
**Version:** 0.1.0
